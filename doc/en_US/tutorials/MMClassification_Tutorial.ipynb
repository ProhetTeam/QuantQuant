{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Tutorials - Classification__\n",
    "\n",
    "QQuant provides multiple quantizers for quantization-aware fine-tuning. This tutorial provides several light demos, which are designed to introduce the overall style of QQuant and make you get started quickly. We assume that the reader has basic concepts of deep learning. \n",
    "\n",
    "QQuant currently supports PyTorch models. \n",
    "\n",
    "## __Turorial : Structure of lowbit classification__\n",
    "\n",
    "The directory structure of lowbit classification is as fllows:\n",
    "\n",
    "```\n",
    "├── demo                   //Jupyter DEMO \n",
    "├── doc                    //Tutorial\n",
    "├── lbitcls                //Core Module\n",
    "│   ├── apis               //Train,Test,Inference API\n",
    "│   ├── core               //Eval, Fp16, and etc\n",
    "│   ├── datasets           //Dataset and Dataloader\n",
    "│   ├── __init__.py         \n",
    "│   ├── models             //Models: Backbone, Neck, Loss, Head\n",
    "│   ├── utils              //Tools\n",
    "│   ├── VERSION            //Version Info\n",
    "│   └── version.py\n",
    "├── README.md\n",
    "├── requirements           //Requirements\n",
    "│   ├── build.txt\n",
    "│   ├── docs.txt\n",
    "│   ├── optional.txt\n",
    "│   ├── readthedocs.txt\n",
    "│   ├── runtime.txt\n",
    "│   └── tests.txt\n",
    "├── setup.py               //Install Python Script\n",
    "├── thirdparty             //Thirdparty\n",
    "│   └── configs            //Running Configure\n",
    "├── tools\n",
    "│   ├── dist_train.sh      //Distribution Training On Brain++\n",
    "│   └── train.py           //Starting Training Script\n",
    "└── work_dirs              //Your Working directory\n",
    "    └── DSQ\n",
    "```\n",
    "\n",
    "<!--\n",
    "- __lbitcls__ contains core modules for training and evaluation like __datasets__ and __models__. For dataset format, we suggest to convert the data format into existing format(ImageNet). __Model__ module is disassembled into backbone, neck, classifier, loss, etc. To support new formats for datasets and models, you could define new classes under their corresponding directory.\n",
    "- __requirements__ \n",
    "- \n",
    "-->\n",
    "## __Turorial : Inference with Quantization Model__\n",
    "\n",
    "This section introduces how to convert a floating-point model to a quantization version and make inferences on a given image. \n",
    "\n",
    "### __Specify the model configuration file__\n",
    "\n",
    "The configuration file determines all arguments related to the experiment, including model structure, dataset, quantization method, etc. These arguments are modularized so that we can customize them respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "from lbitcls import __version__\n",
    "config = 'config.py'\n",
    "config = mmcv.Config.fromfile(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Build quantized model from the configuration file__\n",
    "\n",
    "Building a network from scratch is usually a tedious process because it consists of several nested layers/modules. When quantifying an existing model, we will not repeat above steps. QQuant provides model transformer which recursively traverse the network structure and replace them with specified quantization layers. In addition to directly using the existing quantification methods, we could also modify them, or customize a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdparty.mtransformer import build_mtransformer\n",
    "from mmcv.runner import load_checkpoint\n",
    "import warnings\n",
    "from lbitcls.models import build_classifier\n",
    "\n",
    "model = build_classifier(config.model)\n",
    "# Quantize the floating-point model\n",
    "if hasattr(config, \"quant_transformer\"):\n",
    "    # Create a quantizer\n",
    "    model_transformer = build_mtransformer(config.quant_transformer)\n",
    "    # Quantize the floating-point model \n",
    "    model = model_transformer(model)\n",
    "# Choose cpu or gpu device for inference \n",
    "device = 'cpu'\n",
    "# Load the checkpoint\n",
    "if config.load_from is not None:\n",
    "    map_loc = 'cpu' if device == 'cpu' else None\n",
    "    checkpoint = load_checkpoint(model, config.load_from, map_location=map_loc)\n",
    "    if 'CLASSES' in checkpoint['meta']:\n",
    "            model.CLASSES = checkpoint['meta']['CLASSES']\n",
    "    else:\n",
    "        from lbitcls.datasets import ImageNet\n",
    "        warnings.simplefilter('once')\n",
    "        warnings.warn('Class names are not saved in the checkpoint\\'s '\n",
    "                        'meta data, use imagenet by default.')\n",
    "        model.CLASSES = ImageNet.CLASSES\n",
    "model.to(device)\n",
    "model.cfg = config\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Prepare data__ \n",
    "\n",
    "Before the data is fed to the network, a pipeline of pre-processing like cropping and normalization are required. These operations are recorded in the configuration file as a dict. In some cases, we may only want to use a single image for debugging instead of the entire dataset. The data pipeline and dataset loader are decoupled for better modularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lbitcls.datasets.pipelines import Compose\n",
    "from mmcv.parallel import collate, scatter\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "img_file = './test.jpg'\n",
    "cfg = model.cfg\n",
    "data = dict(img_info=dict(filename=img_file), img_prefix=None)\n",
    "# Extract the test transformation pipline from the config file\n",
    "test_pipeline = Compose(cfg.data.test.pipeline)\n",
    "# Transform raw data to the specified format\n",
    "data = test_pipeline(data)\n",
    "data = collate([data], samples_per_gpu=1)\n",
    "device = next(model.parameters()).device  # model device\n",
    "if next(model.parameters()).is_cuda:\n",
    "    # scatter to specified GPU\n",
    "    data = scatter(data, [device])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Forward the quantized model__\n",
    "\n",
    "Regardless of whether the model is quantified or not, the process of inference on images is unchanged, as same as other standard operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    scores = model(return_loss=False, **data)\n",
    "    pred_score = np.max(scores, axis=1)[0]\n",
    "    pred_label = np.argmax(scores, axis=1)[0]\n",
    "    result = {'pred_label': pred_label, 'pred_score': float(pred_score)}\n",
    "result['pred_class'] = model.CLASSES[result['pred_label']]\n",
    "# Show the predicted results\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cls_quant_config## __Turorials 2: Start Training and Evaluation__\n",
    "\n",
    "**QAT** (Quantization aware training) simulates the process of low-bit model training by inserting quantization nodes in some network modules. To practice a complete QAT approach, there are the following steps:\n",
    "\n",
    "-  Confirm the configuration file\n",
    "-  Train and test quantization model \n",
    "-  Analyze quantization performance\n",
    "\n",
    "### __Confirm the config file__\n",
    "\n",
    "If you just want to reproduce the existing quantization methods, you only need to learn the composition of the config file and be able to set it correctly. Generally, we don’t need to write a new config file from scratch, but inherit the existing standard template and modify some of the components. \n",
    "\n",
    "Next, we will establish a preliminary understanding about config file by reading a complete config example of Differentiable Soft Quantization (DSQ).\n",
    "\n",
    "\n",
    "#### __Config Name__\n",
    "\n",
    "The file path is: ```\"lowbit_classification/thirdparty/cls_quant_config/DSQ/res50/config2_res50_dsq_m1_16_2w2f.py\"```.\n",
    "The directory where the config file is located has a two-level structure, the first level is named by QAT method, and the second layer is named by backbone. The name of config file is determined by a naming rules:\n",
    "\n",
    "```config(number)_res18(backbone)_UQ(qat_method)_m4(machine number)_64(sample_per_gpu)_2w2f(quant_bit).py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  - ```(number)```: experiment number.\n",
    "  - ```(backbone)```: backbone type, like ```res50```, ```mobilenet```.\n",
    "  - ```(qat_method)```: quantization method type like ```LSQ```, ```DSQ```, ```DoReFa```.\n",
    "  - ```(machine number)```: machine number.\n",
    "  - ```(sample_per_gpu)```:  batch size per GPU, like ```64```, ```128```.\n",
    "  - ```(quant_bit)```: quantization bit of weights and activations, like ```2w2f```, ```4w4f```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Model Structure__"
   ]
  },
  {
   "source": [
    "```python=\n",
    "model = dict(\n",
    "    type='ImageClassifier',  # Type of classifier\n",
    "    backbone=dict(\n",
    "        type='ResNet',  #  Type of backbone\n",
    "        depth=50,  # Depth of backbone\n",
    "        num_stages=4,  # Number of stages of the backbone\n",
    "        out_indices=(3, ),  # The index of output feature maps produced in each stages\n",
    "        style='pytorch'),  # The style of backbone\n",
    "    neck=dict(type='GlobalAveragePooling'),  # Type of neck of model \n",
    "    head=dict(\n",
    "        type='LinearClsHead',  # Type of head used for classification\n",
    "        num_classes=1000,  # The number of classes \n",
    "        in_channels=2048,  # Input channels for head\n",
    "        loss=dict(type='CrossEntropyLoss', loss_weight=1.0),  # Type of loss for classification\n",
    "        topk=(1, 5),  # Top-k accuracy\n",
    "    ))\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Dataset Setting__ "
   ]
  },
  {
   "source": [
    "```python=\n",
    "dataset_type = 'ImageNetV1'  # Type of dataset\n",
    "img_norm_cfg = dict(  # Image Normalization config\n",
    "    mean=[0.485, 0.456, 0.406],  # Mean values used to normalize the images\n",
    "    std=[0.229, 0.224, 0.225],  # Standard variance used to normalize the images\n",
    "    to_rgb=True)  #  Whether to adjust the channel orders to rgb orders\n",
    "train_pipeline = [  # Training pipline\n",
    "    dict(type='LoadImageFromNori'),  # Type of load images\n",
    "    dict(type='RandomResizedCrop', size=224),  # Augmentation pipeline that crop the images\n",
    "    dict(type='RandomFlip',  # Augmentation pipeline that flip the images randomly\n",
    "    flip_prob=0.5,  # Probability to perform a flip operation\n",
    "    direction='horizontal'),  # Flip direction\n",
    "    dict(type='Normalize',  # Augmentation pipeline that normalize the images\n",
    "    **img_norm_cfg),  # Normalization parameters are read from img_norm_cfg\n",
    "    dict(type='ImageToTensor', keys=['img']),  # Convert image to tensor\n",
    "    dict(type='ToTensor', keys=['gt_label']),  # Convert label to tensor\n",
    "    dict(type='Collect', keys=['img', 'gt_label'])  # Collect pipeline that collect necessary keys\n",
    "]\n",
    "test_pipeline = [  # Test pipline\n",
    "    dict(type='LoadImageFromNori'),  # Type of load images\n",
    "    dict(type='Resize', size=(256, -1)),  # Augmentation pipeline that resize the images\n",
    "    dict(type='CenterCrop', crop_size=224),  # Augmentation pipeline that crop the images\n",
    "    dict(type='Normalize',  # Augmentation pipeline that normalize the images\n",
    "    **img_norm_cfg),  # Normalization parameters are read from img_norm_cfg\n",
    "    dict(type='ImageToTensor', keys=['img']),  # Convert image to tensor\n",
    "    dict(type='Collect', keys=['img'])  # Collect pipeline that collect necessary keys\n",
    "]\n",
    "\n",
    "data = dict(  \n",
    "    samples_per_gpu=16,  # Batch size of a single GPU\n",
    "    workers_per_gpu=3,  # Worker to pre-fetch data for each single GPU\n",
    "    train=dict(\n",
    "        type=dataset_type,\n",
    "        data_prefix= None,\n",
    "        ann_file=\"/data/workspace/dataset/imagenet/imagenet.train.nori.list\",\n",
    "        pipeline=train_pipeline),\n",
    "    val=dict(\n",
    "        type=dataset_type,\n",
    "        data_prefix=None,\n",
    "        ann_file=\"/data/workspace/dataset/imagenet/imagenet.val.nori.list\",\n",
    "        pipeline=test_pipeline),\n",
    "    test=dict(\n",
    "        # replace `data/val` with `data/test` for standard test\n",
    "        type=dataset_type,\n",
    "        data_prefix= None,\n",
    "        ann_file=\"/data/workspace/dataset/imagenet /imagenet.val.nori.list\",\n",
    "        pipeline=test_pipeline))\n",
    "evaluation = dict(interval=2, metric='accuracy')\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Quantization Setting__"
   ]
  },
  {
   "source": [
    "```python=\n",
    "quant_transformer = dict(  \n",
    "    type = \"mTransformerV2\",  # Type of quantization transformer \n",
    "    quan_policy=dict(  \n",
    "        Conv2d=dict(type='DSQConv',  # DSQ quant layer used to replace standard conv layer\n",
    "        num_bit_w=2,  # Bit number of weight\n",
    "        num_bit_a=2,  # Bit number of activation\n",
    "        bSetQ=True),  # Switch of quantization\n",
    "        Linear=dict(type='DSQLinear',  # DSQ quant layer used to replace standard linear layer\n",
    "        num_bit_w=2,  # Bit number of weight\n",
    "        num_bit_a=2)  # Bit number of activation\n",
    "        ),\n",
    "    special_layers = dict(  # Special layers that adopt different quant policy\n",
    "        layers_name = [  # Names of special layers \n",
    "            'backbone.conv1',  \n",
    "            'head.fc'],\n",
    "        convert_type = [dict(\n",
    "        type='DSQConv', # DSQ quant layer used to replace first conv layer of backbone\n",
    "        num_bit_w=8, num_bit_a=8, bSetQ=True, quant_activation=False),\n",
    "        dict(type='DSQLinear', # DSQ quant layer used to replace fc layer of head\n",
    "        num_bit_w=8, num_bit_a=8)]\n",
    "        )\n",
    ")\n",
    "````"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Optimizer__"
   ]
  },
  {
   "source": [
    "```python=\n",
    "num_nodes = 1  # Number of machine\n",
    "optimizer = dict(type='SGD',  # Type of optimizers\n",
    "    lr=0.001 * num_nodes,  # Learning rate of optimizers\n",
    "    momentum=0.9,  # Momentum\n",
    "    weight_decay=0.0001)  # Weight decay \n",
    "optimizer_config = dict(grad_clip=None)  # Config used to build the optimizer hook\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Learning Policy__"
   ]
  },
  {
   "source": [
    "```python=\n",
    "lr_config = dict(  # Learning rate scheduler config used to register LrUpdater hook\n",
    "    policy='step',  # The policy of scheduler, also support CosineAnnealing, Cyclic, etc.\n",
    "#    warmup='linear',  # The warmup policy, also support `exp` and `constant`.\n",
    "#    warmup_iters=3000,  # The number of iterations for warmup\n",
    "#    warmup_ratio=0.25,  # The ratio of the starting learning rate used for warmup\n",
    "    step=[30, 60, 90])  # Steps to adjust the learning rate\n",
    "total_epochs = 100  # The number of total epochs\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Logger Setting__"
   ]
  },
  {
   "source": [
    "```python=\n",
    "log_level = 'INFO'  # The level of logging\n",
    "log_config = dict(  # config to register logger hook\n",
    "    interval=200,  # Interval to print the log\n",
    "    hooks=[  # The logger used to record the training process.\n",
    "        dict(type='TextLoggerHook'),  \n",
    "#        dict(type='TensorboardLoggerHook')\n",
    "])\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### __Others__\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```python=\n",
    "checkpoint_config = dict(interval=2)  # The interval of checkpoint saving\n",
    "dist_params = dict(backend='nccl')  # Parameters to setup distributed training\n",
    "work_dir = '/data/workspace/lowbit_classification/workdirs/DSQ/res50/config2_res50_dsq_m1_16_2w2f'  # Directory to save the model checkpoints and relevant logs\n",
    "workflow = [('train', 1)]  # Workflow for runner. Format of workflow: [(mode1, epochs), (mode2, epochs), ...].\n",
    "load_from = './thirdparty/modelzoo/res50.pth'  # load models as a pre-trained model from a given path.  This will not resume training.\n",
    "resume_from = None  # Resume checkpoints from a given path, the training will be resumed from the epoch when the checkpoint's is saved.\n",
    "cpu_only=False  # Whether to run only on cpu\n",
    "find_unused_parameters = True  # Whether to find unused parameters\n",
    "sycbn = False  # Whether to do synchronization of BN statistics\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Train and test quantization model__\n",
    "\n",
    "After completing the above step, all the details about the experiment have been determined. To train a model with this config file, we could simply run"
   ]
  },
  {
   "source": [
    "```shell=\n",
    "cd lowbit_classification\n",
    "python tools/train.py  thirdparty/cls_quant_config/DSQ/res50/config2_res50_dsq_m1_16_2w2f.py\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "Next, the program will output log information with the following format, which can be viewed under ```work_dir``` directory."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```shell=\n",
    "2021-07-21 21:21:52,381 - lbitcls - INFO - load checkpoint from /thirdparty/modelzoo/res50.pth\n",
    "2021-07-21 21:21:52,382 - lbitcls - INFO - Use load_from_local loader\n",
    "2021-07-21 21:21:52,959 - lbitcls - INFO - Start running, host: ***, work_dir: /data/workspace/lowbit_classification/workdirs/DSQ/res18/config_debug\n",
    "2021-07-21 21:21:52,959 - lbitcls - INFO - workflow: [('train', 1)], max: 100 epochs\n",
    "2021-07-21 21:26:47,622 - lbitcls - INFO - Epoch [1][200/626]\tlr: 1.000e-03, eta: 1 day, 1:32:11, time: 1.473, data_time: 0.449, memory: 10283, loss: 1.3465, top-1: 68.2227, top-5: 86.7495\n",
    "2021-07-21 21:30:14,855 - lbitcls - INFO - Epoch [1][400/626]\tlr: 1.000e-03, eta: 21:40:43, time: 1.036, data_time: 0.024, memory: 10283, loss: 1.3443, top-1: 68.2712, top-5: 86.7048\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2021-07-21 21:21:52,381 - lbitcls - INFO - load checkpoint from /thirdparty/modelzoo/res50.pth\n",
    "2021-07-21 21:21:52,382 - lbitcls - INFO - Use load_from_local loader\n",
    "2021-07-21 21:21:52,959 - lbitcls - INFO - Start running, host: ***, work_dir: /data/workspace/lowbit_classification/workdirs/DSQ/res18/config5_res18_dsq_m2_128_4w4f\n",
    "2021-07-21 21:21:52,959 - lbitcls - INFO - workflow: [('train', 1)], max: 100 epochs\n",
    "2021-07-21 21:26:47,622 - lbitcls - INFO - Epoch [1][200/626]\tlr: 1.000e-03, eta: 1 day, 1:32:11, time: 1.473, data_time: 0.449, memory: 10283, loss: 1.3465, top-1: 68.2227, top-5: 86.7495\n",
    "2021-07-21 21:30:14,855 - lbitcls - INFO - Epoch [1][400/626]\tlr: 1.000e-03, eta: 21:40:43, time: 1.036, data_time: 0.024, memory: 10283, loss: 1.3443, top-1: 68.2712, top-5: 86.7048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Analyze quantization performance__\n",
    "\n",
    "QQuant provides model analysis API for further experiments. To explore more about the properties of the quantization model and how it differs from the standard model, we could use this script"
   ]
  },
  {
   "source": [
    "```shell=\n",
    "python tools/model_analysis_tool.py \\\n",
    "    ${IMAGE_FILE} \\\n",
    "    ${FLOAT_CONFIG_FILE} \\\n",
    "    ${INT_CONFIG_FILE} \\\n",
    "    ${FLOAT_CHECKPOINT_FILE} \\\n",
    "    ${INT_CHECKPOINT_FILE} \\\n",
    "    [--device ${GPU_ID}] \\\n",
    "    [--save-path ${HTML_SAVE_PATH}]\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python tools/model_analysis_tool.py \\\n",
    "    ${IMAGE_FILE} \\\n",
    "    ${FLOAT_CONFIG_FILE} \\\n",
    "    ${INT_CONFIG_FILE} \\\n",
    "    ${FLOAT_CHECKPOINT_FILE} \\\n",
    "    ${INT_CHECKPOINT_FILE} \\\n",
    "    [--device ${GPU_ID}] \\\n",
    "    [--save-path ${HTML_SAVE_PATH}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "source": [
    "```shell=\n",
    "python tools/model_analysis_tool.py \\\n",
    "    doc/tutorials/test.jpg \\\n",
    "    thirdparty/configs/DSQ/res50/config1_res50_dsq_m1_16_32w32f.py \\\n",
    "    thirdparty/configs/DSQ/res50/config2_res50_dsq_m1_16_2w2f.py \\\n",
    "    ./thirdparty/modelzoo/res50.pth \\\n",
    "    /data/workspace/lowbit_classification/workdirs/DSQ/res50/config2_res50_dsq_m1_16_2w2f/latest.pth \\\n",
    "    gpu:0 \\\n",
    "    ./model_analysis.html\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python tools/model_analysis_tool.py \\\n",
    "    doc/tutorials/test.jpg \\\n",
    "    thirdparty/configs/DSQ/res50/config1_res50_dsq_m1_16_32w32f.py \\\n",
    "    thirdparty/configs/DSQ/res50/config2_res50_dsq_m1_16_2w2f.py \\\n",
    "    ./thirdparty/modelzoo/res50.pth \\\n",
    "    /data/workspace/lowbit_classification/workdirs/DSQ/res50/config2_res50_dsq_m1_16_2w2f/latest.pth \\\n",
    "    gpu:0 \\\n",
    "    ./model_analysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Turorial: Self-Define Quant Transformer__\n",
    "\n",
    "To customize a new QAT approach, there are the following steps:\n",
    "\n",
    "-  Define a new quantization class\n",
    "-  Import the module\n",
    "-  Modify the quantization setting in config file\n",
    "\n",
    "### __Define a new quantization class__\n",
    "\n",
    "Suppose we want to devolop a new quantization method DSQv2. First, create a new folder ```DSQv2``` under ```QuanTransformer/quantrans/quantops'```, and create a new file ```QuanTransformer/quantrans/quantops/DSQv2/DSQConvV2.py'```."
   ]
  },
  {
   "source": [
    "```python=\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ..builder import QUANLAYERS\n",
    "\n",
    "@QUANLAYERS.register_module()\n",
    "class DSQConvV2(nn.Conv2d):\n",
    "    def __init__(self, *args):\n",
    "\n",
    "    def quantize_(self, x, *args):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after defining the class, we add a decorator function: \n",
    "```@QUANLAYERS.register_module()```.\n",
    "Then,  we import this module in ```/QuanTransformer/quantrans/quantops/DSQv2/__init__.py'```: "
   ]
  },
  {
   "source": [
    "```python=\n",
    "from .DSQConvV2 import DSQConvV2\n",
    "\n",
    "__all__=['DSQConvV2']\n",
    "```\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "this process is equivalent to excuting  ```DSQConvV2 = QUANLAYERS.register_module(DSQConvV2()) ```.\n",
    "\n",
    "### __Modify the quantization setting in config file__\n",
    "\n",
    "\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "```\n",
    "quant_transformer = dict(\n",
    "    type = \"mTransformerV2\",\n",
    "    quan_policy=dict(\n",
    "        Conv2d=dict(type='DSQConvV2', num_bit_w=3, num_bit_a=3, bSetQ=True),\n",
    "        ),\n",
    "    special_layers = dict(\n",
    "        layers_name = [\n",
    "            'backbone.conv1',\n",
    "            'head.fc'],\n",
    "        convert_type = [dict(type='DSQConv', num_bit_w=8, num_bit_a=8, bSetQ=True, quant_activation=False),\n",
    "                        dict(type='DSQLinear', num_bit_w=8, num_bit_a=8)]\n",
    "        )\n",
    ")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __How the new modules are used to quantify the model__\n",
    "\n",
    "Using quantization setting as a argument, we define a model transformer:\n",
    "\n",
    "```model_transformer = build_mtransformer(cfg.quant_transformer)```\n",
    "\n",
    "In ```__init__``` function, the dict of quantization setting is assigned to ```self.register_dict```:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python=\n",
    "class mTransformerV2(Basemtransformer, nn.Module):\n",
    "    def __init__(self, \n",
    "                 quan_policy = dict(),\n",
    "                 special_layers = None,\n",
    "                 **kwargs):\n",
    "        super(mTransformerV2, self).__init__()\n",
    "        self.special_layers = special_layers\n",
    "\n",
    "        self.register_dict = OrderedDict()\n",
    "        for key, value in quan_policy.items():\n",
    "            assert(hasattr(nn, key))\n",
    "            self.register_dict[getattr(nn, key)] = value\n",
    "        self.layer_idx = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the transformer convert a standard model into a quantization model:"
   ]
  },
  {
   "source": [
    "```python=\n",
    "model = model_transformer(model, logger = logger)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each module, the transformer first gets the name and check whether it exists in ```self.register_dict```:"
   ]
  },
  {
   "source": [
    "```python=\n",
    "if type(current_layer) not in self.register_dict:\n",
    "    continue\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the current layer can be converted, first extract the parameters of the original layer ```new_kwargs```, "
   ]
  },
  {
   "source": [
    "```python=\n",
    "## 1. get parameters\n",
    "sig = inspect.signature(type(getattr(model, module_name)))\n",
    "new_kwargs = {}\n",
    "for key in sig.parameters:\n",
    "    if sig.parameters[key].default != inspect.Parameter.empty:\n",
    "        continue\n",
    "    assert(hasattr(current_layer, key))\n",
    "    new_kwargs[key] = getattr(current_layer, key)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and get the corresponding quantization arguments ```quan_args``` according to the layer name.\n",
    "\n",
    "These arguments are combined to build a new quant layer. The weights of current layer is merged to quant layer, so the quant layer can use it for operations like convolution. Finally, the quantization layer replaces the current layer in the model.\n"
   ]
  },
  {
   "source": [
    "```python=\n",
    "## 2. Special layers or Normal layer\n",
    "if current_layer_name in self.special_layers.layers_name:\n",
    "    idx = self.special_layers.layers_name.index(current_layer_name)\n",
    "    quan_args = self.special_layers.convert_type[idx]\n",
    "else:\n",
    "    quan_args = self.register_dict[type(current_layer)]\n",
    "    new_kwargs = {**quan_args, **new_kwargs}\n",
    "    new_quan_layer = build_quanlayer(new_kwargs)\n",
    "    dict_merge(new_quan_layer.__dict__, current_layer.__dict__)\n",
    "    setattr(model, module_name, new_quan_layer)\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}